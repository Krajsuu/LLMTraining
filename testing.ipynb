{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c27fe0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\monik\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c8a7a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2158105",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer: AutoTokenizer = None\n",
    "model: PeftModel = None\n",
    "model_loaded: bool = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de09d0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    global tokenizer, model, model_loaded, load_error\n",
    "    \n",
    "    if model_loaded:\n",
    "        return \n",
    "    \n",
    "    try:\n",
    "        model_path = \"./joke_model_output/final_model\"\n",
    "        base_model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "        if not os.path.exists(model_path):\n",
    "            load_error = f\"Nie znaleziono modelu w {model_path}\"\n",
    "            return load_error\n",
    "        \n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3  # GB\n",
    "            print(f\"Dostępna pamięć GPU: {gpu_memory:.1f} GB\")\n",
    "            \n",
    "            if gpu_memory < 8:  \n",
    "                device_map = \"cpu\"\n",
    "                torch_dtype = torch.float32\n",
    "                offload_folder = None\n",
    "            elif gpu_memory < 16:  \n",
    "                device_map = \"auto\"\n",
    "                torch_dtype = torch.float16\n",
    "                offload_folder = \"./model_offload\"\n",
    "            else: \n",
    "\n",
    "                device_map = \"auto\"\n",
    "                torch_dtype = torch.float16\n",
    "                offload_folder = None\n",
    "        else:\n",
    "            device_map = \"cpu\"\n",
    "            torch_dtype = torch.float32\n",
    "            offload_folder = None\n",
    "        \n",
    "        \n",
    "        load_kwargs = {\n",
    "            \"torch_dtype\": torch_dtype,\n",
    "            \"device_map\": device_map,\n",
    "            \"trust_remote_code\": True,\n",
    "            \"low_cpu_mem_usage\": True\n",
    "        }\n",
    "        \n",
    "        if offload_folder:\n",
    "            load_kwargs[\"offload_folder\"] = offload_folder\n",
    "            load_kwargs[\"offload_state_dict\"] = True\n",
    "        \n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_name,\n",
    "            **load_kwargs\n",
    "        )\n",
    "\n",
    "        model = PeftModel.from_pretrained(base_model, model_path)\n",
    "\n",
    "        \n",
    "    except Exception as e:\n",
    "        load_error = f\"Błąd ładowania modelu: {str(e)}\"\n",
    "        model = None\n",
    "        tokenizer = None\n",
    "        model_loaded = False\n",
    "        print(load_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "742ad882",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_joke(prompt: str, temperature: float = 0.7, max_length: int = 150) -> str:\n",
    "    global tokenizer, model, model_loaded\n",
    "    if not model_loaded:\n",
    "        print(\"Generating...\")\n",
    "        load_model()\n",
    "    if not prompt.strip():\n",
    "        return \"Send prompt. Please.\"\n",
    "    \n",
    "    '''\n",
    "    message = [\n",
    "        {\n",
    "            \"role\" : \"system\",\n",
    "            \"content\" : \"Jesteś pomocnym asystentem, który opowiada śmieszne polskie dowcipy. Odpowiadasz tylko żarty bez dodatkowych komentarzy. Nie uzywaj wulgsyzmow.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\" : \"user\",\n",
    "            \"content\" : prompt\n",
    "        }\n",
    "    ]'''\n",
    "    system_msg = \"Jesteś pomocnym asystentem, który opowiada śmieszne polskie dowcipy. Odpowiadasz tylko dowcipem, bez dodatkowych komentarzy. Nie używaj wulgaryzmów\"\n",
    "        \n",
    "    formatted_prompt = f\"System: {system_msg}\\nUser: {prompt}\\nAssistant:\"\n",
    "\n",
    "    '''formatted_prompt = tokenizer.apply_chat_template(\n",
    "        message,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt = True \n",
    "    )'''\n",
    "    inputs = tokenizer(\n",
    "            formatted_prompt, \n",
    "            return_tensors=\"pt\", \n",
    "            truncation=True, \n",
    "            max_length=200,  \n",
    "            add_special_tokens=True\n",
    "        )\n",
    "    device = model.device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                input_ids=inputs['input_ids'],\n",
    "                attention_mask=inputs.get('attention_mask'),\n",
    "                max_new_tokens=min(max_length, 100),  \n",
    "                temperature=max(0.3, min(temperature, 1.0)),  \n",
    "                do_sample=True,\n",
    "                top_p=0.8,  \n",
    "                top_k=40,   \n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                repetition_penalty=1.2,  \n",
    "                num_return_sequences=1,\n",
    "                early_stopping=True,  \n",
    "                no_repeat_ngram_size=2  \n",
    "            )\n",
    "\n",
    "    input_length = inputs['input_ids'].shape[1]\n",
    "    generated_tokens = outputs[0][input_length:]\n",
    "    generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "    result = generated_text.strip()\n",
    "        \n",
    "    prefixes_to_remove = [\"Assistant:\", \"Asystent:\", \"System:\", \"User:\", \"Użytkownik:\"]\n",
    "    for prefix in prefixes_to_remove:\n",
    "        if result.startswith(prefix):\n",
    "            result = result[len(prefix):].strip()\n",
    "        \n",
    "    end_markers = ['\\nUser:', '\\nSystem:', '\\nAssistant:', '\\nUżytkownik:']\n",
    "    for marker in end_markers:\n",
    "        if marker in result:\n",
    "            result = result.split(marker)[0].strip()\n",
    "    if len(result.split('\\n')) > 3: \n",
    "        lines = result.split('\\n')[:2] \n",
    "        result = '\\n'.join(lines)\n",
    "        \n",
    "    if '.' in result:\n",
    "        sentences = result.split('.')\n",
    "        if len(sentences) > 1:\n",
    "            result = '. '.join(sentences[:2]) + '.'\n",
    "        \n",
    "\n",
    "    result = result.strip()\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "476490e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_interface() :\n",
    "    with gr.Blocks(title=\"Generator Dowcipów\", theme=gr.themes.Soft()) as app:\n",
    "        gr.Markdown(\"# Generator Polskich Dowcipów\")\n",
    "        gr.Markdown(\"*Wytrenowany model do generowania śmiesznych polskich dowcipów*\")\n",
    "        \n",
    "        \n",
    "        load_model_btn = gr.Button(\"Załaduj Model\", variant=\"secondary\")\n",
    "        \n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=1):\n",
    "                prompt = gr.Textbox(\n",
    "                    label=\"Prompt\",\n",
    "                    placeholder=\"Opowiedz mi polski dowcip...\",\n",
    "                    value=\"Opowiedz mi polski dowcip\",\n",
    "                    lines=3\n",
    "                )\n",
    "                \n",
    "                with gr.Row():\n",
    "                    temperature = gr.Slider(\n",
    "                        minimum=0.1, \n",
    "                        maximum=2.0, \n",
    "                        value=0.7, \n",
    "                        step=0.1,\n",
    "                        label=\"Temperature (kreatywność)\"\n",
    "                    )\n",
    "                    max_length = gr.Slider(\n",
    "                        minimum=50, \n",
    "                        maximum=300, \n",
    "                        value=150, \n",
    "                        step=10,\n",
    "                        label=\"Max Length\"\n",
    "                    )\n",
    "                \n",
    "                with gr.Row():\n",
    "                    generate_btn = gr.Button(\"Generuj Dowcip\", variant=\"primary\", size=\"lg\")\n",
    "                    clear_btn = gr.Button(\"Wyczyść\", size=\"lg\")\n",
    "            \n",
    "            with gr.Column(scale=1):\n",
    "                output = gr.Textbox(\n",
    "                    label=\"Dowcip\",\n",
    "                    lines=12,\n",
    "                    interactive=False,\n",
    "                    placeholder=\"Tutaj pojawi się dowcip...\"\n",
    "                )\n",
    "        \n",
    "        # Przykłady\n",
    "        gr.Examples(\n",
    "            examples=[\n",
    "                [\"Opowiedz mi polski dowcip\", 0.7, 150],\n",
    "                [\"Znasz jakiś dobry żart?\", 0.8, 180], \n",
    "                [\"Powiedz dowcip o programistach\", 0.6, 120],\n",
    "                [\"Opowiedz coś śmiesznego o kotach\", 0.7, 200],\n",
    "                [\"Masz jakiś żart o pracy?\", 0.7, 150]\n",
    "            ],\n",
    "            inputs=[prompt, temperature, max_length]\n",
    "        )\n",
    "        \n",
    "        # Przypisz funkcje\n",
    "        load_model_btn.click(\n",
    "            fn=load_model,\n",
    "            outputs=status_display\n",
    "        )\n",
    "        \n",
    "        generate_btn.click(\n",
    "            fn=generate_joke,\n",
    "            inputs=[prompt, temperature, max_length],\n",
    "            outputs=output\n",
    "        )\n",
    "        \n",
    "        clear_btn.click(\n",
    "            fn=lambda: (\"\", \"\"),\n",
    "            outputs=[prompt, output]\n",
    "        )\n",
    "    \n",
    "    return app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f5bae8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "login(token=os.getenv(\"HF_TOKEN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34c9d7cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'check_model_status' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m app = \u001b[43mgenerate_interface\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m app.launch()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mgenerate_interface\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      4\u001b[39m gr.Markdown(\u001b[33m\"\u001b[39m\u001b[33m*Wytrenowany model do generowania śmiesznych polskich dowcipów*\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Status modelu\u001b[39;00m\n\u001b[32m      7\u001b[39m status_display = gr.Textbox(\n\u001b[32m      8\u001b[39m     label=\u001b[33m\"\u001b[39m\u001b[33mStatus Modelu\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     value=\u001b[43mcheck_model_status\u001b[49m(),\n\u001b[32m     10\u001b[39m     interactive=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     11\u001b[39m )\n\u001b[32m     13\u001b[39m load_model_btn = gr.Button(\u001b[33m\"\u001b[39m\u001b[33mZaładuj Model\u001b[39m\u001b[33m\"\u001b[39m, variant=\u001b[33m\"\u001b[39m\u001b[33msecondary\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m gr.Row():\n",
      "\u001b[31mNameError\u001b[39m: name 'check_model_status' is not defined"
     ]
    }
   ],
   "source": [
    "app = generate_interface()\n",
    "app.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
